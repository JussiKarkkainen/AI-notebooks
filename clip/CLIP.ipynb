{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC"
      ],
      "metadata": {
        "id": "-CXFjnBgZuZT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim, image_resolution, vision_layers, vision_width, vision_heads, \n",
        "                 vision_patch_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=vision_width, kernel_size=vision_patch_size,\n",
        "                               stride=vision_patch_size, bias=False)\n",
        "        scale = vision_width ** -0.5\n",
        "        self.cls = nn.Parameter(scale * torch.randn(vision_width))\n",
        "        self.pos_embedding = nn.Parameter(scale * torch.randn((image_resolution // vision_patch_size) ** 2 + 1, vision_width))\n",
        "        self.ln1 = nn.LayerNorm(vision_width)\n",
        "        self.transformer = Transformer(vision_layers, vision_width, vision_heads)\n",
        "        self.ln2 = nn.LayerNorm(vision_width)\n",
        "        self.projection = nn.Parameter(scale * torch.randn(vision_width, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = torch.cat([self.cls.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
        "        x = x + self.pos_embedding.to(x.dtype)\n",
        "        x = self.ln1(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln2(x)\n",
        "        if self.projection:\n",
        "            x = x @ self.projection\n",
        "        return x"
      ],
      "metadata": {
        "id": "Af9YAmSGXmaE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, width, heads, attention_mask=None):\n",
        "        super().__init__()\n",
        "        # GPT-2 Attention block\n",
        "        # Masked MultiHeadAttnetion -> LayerNorm -> FF -> LayerNorm\n",
        "        self.attn = nn.MultiHeadAttention(width, head)\n",
        "        self.ln1 = nn.LayerNorm(width)\n",
        "        self.mlp = nn.Sequential([nn.Linear(width, width*4), nn.GELU(),\n",
        "                                  nn.Linear(width*4, width)])\n",
        "        self.ln2 = nn.LayerNorm(width)\n",
        "        self.attention_mask = attention_mask\n",
        "    \n",
        "    def attention(self, x):\n",
        "        attn_mask = self.attention_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
        "        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention(self.ln1(x))\n",
        "        return self.mlp(self.ln2(x))"
      ],
      "metadata": {
        "id": "K6Q-MaEIOsBf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, transformer_layers, transformer_width, transformer_heads, attention_mask=None):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential([AttentionBlock(transformer_width, transformer_heads, attention_mask) \n",
        "                                     for _ in range(transformer_layers)])\n",
        "        self.width = transformer_width\n",
        "        self.layers = transformer_layers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)"
      ],
      "metadata": {
        "id": "CrTTX_BTXbiw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP(nn.Module):\n",
        "    def __init__(self, embed_dim, image_resolution, vision_layers, vision_width, \n",
        "                 vision_patch_size, context_length, vocab_size, transformer_width,\n",
        "                 transformer_heads, transformer_layers):\n",
        "        super().__init__()\n",
        "        vision_heads = vision_width // 64\n",
        "        self.vision_encoder = VisionTransformer(embed_dim, image_resolution, vision_layers, vision_width,\n",
        "                                                vision_heads, vision_patch_size)\n",
        "        #self.text_encoder = Transformer(transformer_layers, transformer_width, transformer_heads)\n",
        "        self.text_encoder = nn.Sequential([AttentionBlock(transformer_width, transformer_heads, attention_mask) \n",
        "                                     for _ in range(transformer_layers)])\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
        "        self.ln = nn.LayerNorm(transformer_width)\n",
        "\n",
        "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.pos_embedding, std=0.01)\n",
        "\n",
        "        proj_std = (self.text_encoder.width ** -0.5) * ((2 * self.text_encoder.layers) ** -0.5)\n",
        "        attn_std = self.text_encoder.width ** -0.5\n",
        "        fc_std = (2 * self.text_encoder.width) ** -0.5\n",
        "        for block in self.text_encoder.resblocks:\n",
        "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
        "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
        "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
        "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
        "\n",
        "        if self.text_projection is not None:\n",
        "            nn.init.normal_(self.text_projection, std=self.text_encoder.width ** -0.5)\n",
        "\n",
        "    def build_attention_mask(self):\n",
        "        mask = torch.empty(self.context_length, self.context_length)\n",
        "        mask.fill_(float(\"-inf\"))\n",
        "        mask.triu_(1) # Returns the upper triangular part of a matrix the other elements of the result tensor out are set to 0.\n",
        "        return mask\n",
        "    \n",
        "    def encode_text(self, x):\n",
        "        x = x + self.pos_embedding(x).type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = self.permute(1, 0, 2)\n",
        "        x = self.ln(x).type(self.dtype)\n",
        "        x = x[torch.arange(x.shape[0]), x.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "    def encode_image(self, x):\n",
        "        return self.vision_encoder(x)\n",
        "    \n",
        "    def forward(self, text, img):\n",
        "        vision_output = self.vision_encoder(img)\n",
        "        text_output = self.text_encoder(text)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        return logits_per_image, logits_per_text"
      ],
      "metadata": {
        "id": "CsmRxrHUW27l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zdfx1ALh1-C",
        "outputId": "01051960-13be-4ad3-cc63-92c88ecb9f42"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-20 17:26:47--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n",
            "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.69, 13.107.213.69, 2620:1ec:bdf::69, ...\n",
            "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353976522 (338M) [application/octet-stream]\n",
            "Saving to: ‘ViT-B-32.pt’\n",
            "\n",
            "ViT-B-32.pt         100%[===================>] 337.58M   221MB/s    in 1.5s    \n",
            "\n",
            "2022-12-20 17:26:49 (221 MB/s) - ‘ViT-B-32.pt’ saved [353976522/353976522]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ViT-B-2.pt"
      ],
      "metadata": {
        "id": "ntqhi8bAoIXy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfCr9-Tbkwwj",
        "outputId": "dd08a44f-34e2-4deb-ccea-2d69b39c3f94"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Dec 16 21:15 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbadHNOb4pU9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/clip.py\n",
        "MODELS = {\n",
        "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
        "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
        "    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n",
        "    \"ViT-B/32_jit\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
        "    \"ViT-B/32\": \"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"\n",
        "}\n",
        "\n",
        "def _convert_image_to_rgb(image):\n",
        "    return image.convert(\"RGB\")\n",
        "\n",
        "def load_params(param_path, device):\n",
        "    def _transform(n_px):\n",
        "        return Compose([\n",
        "            Resize(n_px, interpolation=BICUBIC),\n",
        "            CenterCrop(n_px),\n",
        "            _convert_image_to_rgb,\n",
        "            ToTensor(),\n",
        "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "        ])\n",
        "    \n",
        "    import urllib\n",
        "    if not os.path.exists(\"ViT-B-2.pt\"):\n",
        "        with urllib.request.urlopen(param_path) as source, open(\"ViT-B-2.pt\", \"wb\") as output:\n",
        "            with tqdm(total=int(source.headers.get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
        "                while True:\n",
        "                    buffer = source.read(8192)\n",
        "                    if not buffer:\n",
        "                        break\n",
        "\n",
        "                    output.write(buffer)\n",
        "                    loop.update(len(buffer))\n",
        "\n",
        "    with open(\"ViT-B-2.pt\", 'rb') as opened_file:\n",
        "        checkpoint = torch.load(opened_file, map_location=\"cpu\")\n",
        "        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
        "            state_dict = checkpoint['state_dict']\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "        if next(iter(state_dict.items()))[0].startswith('module'):\n",
        "            state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
        "\n",
        "    print(type(state_dict))\n",
        "    raise Exception(\"here\")\n",
        "    vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
        "    vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
        "    vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
        "    grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
        "    image_resolution = vision_patch_size * 32\n",
        "\n",
        "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
        "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
        "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
        "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
        "    transformer_heads = transformer_width // 64\n",
        "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n",
        "    \n",
        "    model = CLIP(embed_dim,\n",
        "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
        "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers)\n",
        "\n",
        "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
        "        if key in state_dict:\n",
        "            del state_dict[key]\n",
        "    \n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.eval()\n",
        "    \n",
        "    return model, _transform(model.visual.input_resolution)\n",
        "\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = load_params(MODELS['ViT-B/32'], device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "# Prepare the inputs\n",
        "image, class_id = cifar100[3637]\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "id": "DhErs0aD1OB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}