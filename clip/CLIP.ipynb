{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC"
      ],
      "metadata": {
        "id": "-CXFjnBgZuZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer:\n",
        "    def __init__(self, embed_dim, image_resolution, vision_layers, vision_width, vision_heads, \n",
        "                 vision_patch_size):\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=vision_width, kernel_size=vision_patch_size,\n",
        "                               stride=vision_patch_size, bias=False)\n",
        "        scale = vision_width ** -0.5\n",
        "        self.cls = nn.Parameter(scale * torch.randn(width))\n",
        "        self.pos_embedding = nn.Parameter(scale * torch.randn((image_resolution // vision_patch_size) ** 2 + 1, vision_width))\n",
        "        self.ln1 = nn.LayerNorm(vision_width)\n",
        "        self.transformer = Transformer(vision_layers, vision_width, vision_heads)\n",
        "        self.ln2 = nn.LayerNorm(vision_width)\n",
        "        self.projection = nn.Parameter(scale * torch.randn(width, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = torch.cat(self.cls.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device, x]), dim=1)\n",
        "        x = x + self.pos_embedding.to(x.dtype)\n",
        "        x = self.ln1(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln2(x)\n",
        "        if self.projection:\n",
        "            x = x @ self.projection\n",
        "        return x"
      ],
      "metadata": {
        "id": "Af9YAmSGXmaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, width, heads, attention_mask=None):\n",
        "        # GPT-2 Attention block\n",
        "        # Masked MultiHeadAttnetion -> LayerNorm -> FF -> LayerNorm\n",
        "        self.attn = nn.MultiHeadAttention(width, head)\n",
        "        self.ln1 = nn.LayerNorm(width)\n",
        "        self.mlp = nn.Sequential([nn.Linear(width, width*4), nn.GeLU(),\n",
        "                                  nn.Linear(width*4, width)])\n",
        "        self.ln2 = nn.LayerNorm(width)\n",
        "        self.attention_mask = attention_mask\n",
        "    \n",
        "    def attention(self, x):\n",
        "        attn_mask = self.attention_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
        "        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention(self.ln1(x))\n",
        "        return self.mlp(self.ln2(x))"
      ],
      "metadata": {
        "id": "K6Q-MaEIOsBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer:\n",
        "    def __init__(self, transformer_layers, transformer_width, transformer_heads, attention_mask=None):\n",
        "        self.blocks = nn.Sequential([AttentionBlock(transformer_width, transformer_heads, attention_mask) \n",
        "                                     for _ in range(transformer_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)"
      ],
      "metadata": {
        "id": "CrTTX_BTXbiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP:\n",
        "    def __init__(self, embed_dim, image_resolution, vision_layers, vision_width, \n",
        "                 vision_patch_size, context_length, vocab_size, transformer_width,\n",
        "                 transformer_heads, transformer_layers):\n",
        "        vision_heads = vision_width // 64\n",
        "        self.vision_encoder = VisionTransformer(embed_dim, image_resolution, vision_layers, vision_width,\n",
        "                                                vision_heads, vision_patch_size)\n",
        "        #self.text_encoder = Transformer(transformer_layers, transformer_width, transformer_heads)\n",
        "        self.text_encoder = nn.Sequential([AttentionBlock(transformer_width, transformer_heads, attention_mask) \n",
        "                                     for _ in range(transformer_layers)])\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
        "        self.ln = nn.LayerNorm(transformer_width)\n",
        "\n",
        "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "    def build_attention_mask(self):\n",
        "        mask = torch.empty(self.context_length, self.context_length)\n",
        "        mask.fill_(float(\"-inf\"))\n",
        "        mask.triu_(1) # Returns the upper triangular part of a matrix the other elements of the result tensor out are set to 0.\n",
        "        return mask\n",
        "    \n",
        "    def encode_text(self, x):\n",
        "        x = x + self.pos_embedding(x).type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = self.permute(1, 0, 2)\n",
        "        x = self.ln(x).type(self.dtype)\n",
        "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection]\n",
        "\n",
        "    def encode_image(self, x):\n",
        "        return self.vision_encoder(x)\n",
        "    \n",
        "    def forward(self, text, img):\n",
        "        vision_output = self.vision_encoder(img)\n",
        "        text_output = self.text_encoder(text)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        return logits_per_image, logits_per_text"
      ],
      "metadata": {
        "id": "CsmRxrHUW27l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbadHNOb4pU9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/clip.py\n",
        "MODELS = {\n",
        "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
        "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
        "    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n",
        "}\n",
        "\n",
        "def _convert_image_to_rgb(image):\n",
        "    return image.convert(\"RGB\")\n",
        "\n",
        "def load_params(param_path, device):\n",
        "    def _transform(n_px):\n",
        "        return Compose([\n",
        "            Resize(n_px, interpolation=BICUBIC),\n",
        "            CenterCrop(n_px),\n",
        "            _convert_image_to_rgb,\n",
        "            ToTensor(),\n",
        "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "        ])\n",
        "    \n",
        "    with open(param_path) as f:\n",
        "        state_dict = torch.load(f, map_location=\"cpu\")\n",
        "\n",
        "    model = CLIP(embed_dim,\n",
        "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
        "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers)\n",
        "\n",
        "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
        "        if key in state_dict:\n",
        "            del state_dict[key]\n",
        "    \n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.eval()\n",
        "    \n",
        "    return model, _transform(model.visual.input_resolution)\n",
        "\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = load_params(MODELS['ViT-B/32'], device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "# Prepare the inputs\n",
        "image, class_id = cifar100[3637]\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "id": "DhErs0aD1OB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}