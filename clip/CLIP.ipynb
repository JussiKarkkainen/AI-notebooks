{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC"
      ],
      "metadata": {
        "id": "-CXFjnBgZuZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAttentionBlock:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "xBcyxswszzkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer:\n",
        "    def __init__(self, ViTconfig):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "Af9YAmSGXmaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer:\n",
        "    def __init__(self, text_config):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "CrTTX_BTXbiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP:\n",
        "    def __init__(self, clip_config):\n",
        "        self.vision_encoder = VisionTransformer()\n",
        "        self.text_encoder = Transformer()\n",
        "\n",
        "    def forward(self, text, img):\n",
        "        vision_output = self.vision_encoder(img)\n",
        "        text_output = self.text_encoder(text)\n",
        "        return vision_output, text_output"
      ],
      "metadata": {
        "id": "CsmRxrHUW27l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbadHNOb4pU9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/clip.py\n",
        "MODELS = {\n",
        "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
        "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
        "    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n",
        "}\n",
        "\n",
        "def _convert_image_to_rgb(image):\n",
        "    return image.convert(\"RGB\")\n",
        "\n",
        "def load_params(param_path, device):\n",
        "    def _transform(n_px):\n",
        "        return Compose([\n",
        "            Resize(n_px, interpolation=BICUBIC),\n",
        "            CenterCrop(n_px),\n",
        "            _convert_image_to_rgb,\n",
        "            ToTensor(),\n",
        "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "        ])\n",
        "    \n",
        "    with open(param_path) as f:\n",
        "        state_dict = torch.load(f, map_location=\"cpu\")\n",
        "\n",
        "    model = CLIP(embed_dim,\n",
        "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
        "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers)\n",
        "\n",
        "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
        "        if key in state_dict:\n",
        "            del state_dict[key]\n",
        "    \n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.eval()\n",
        "    \n",
        "    return model, _transform(model.visual.input_resolution)\n",
        "\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = load_params(MODELS['ViT-B/32'], device)\n",
        "\n",
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "# Prepare the inputs\n",
        "image, class_id = cifar100[3637]\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ]
    }
  ]
}