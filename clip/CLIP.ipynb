{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC"
      ],
      "metadata": {
        "id": "-CXFjnBgZuZT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim, image_resolution, vision_layers, vision_width, vision_heads, \n",
        "                 vision_patch_size):\n",
        "        super().__init__()\n",
        "        self.input_resolution = image_resolution\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=vision_width, kernel_size=vision_patch_size,\n",
        "                               stride=vision_patch_size, bias=False)\n",
        "        scale = vision_width ** -0.5\n",
        "        self.class_embedding = nn.Parameter(scale * torch.randn(vision_width))\n",
        "        self.positional_embedding = nn.Parameter(scale * torch.randn((image_resolution // vision_patch_size) ** 2 + 1, vision_width))\n",
        "        self.ln_pre = nn.LayerNorm(vision_width)\n",
        "        self.transformer = Transformer(vision_layers, vision_width, vision_heads)\n",
        "        self.ln_post = nn.LayerNorm(vision_width)\n",
        "        self.proj = nn.Parameter(scale * torch.randn(vision_width, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
        "        x = x + self.positional_embedding.to(x.dtype)\n",
        "        x = self.ln_pre(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln_post(x)\n",
        "        if self.proj is not None:\n",
        "            x = x @ self.proj\n",
        "        return x"
      ],
      "metadata": {
        "id": "Af9YAmSGXmaE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, width, heads, attention_mask=None):\n",
        "        super().__init__()\n",
        "        # GPT-2 Attention block\n",
        "        # Masked MultiHeadAttnetion -> LayerNorm -> FF -> LayerNorm\n",
        "        self.attn = nn.MultiheadAttention(width, heads)\n",
        "        self.ln_1 = nn.LayerNorm(width)\n",
        "        self.mlp = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(width, width * 4)),\n",
        "            (\"gelu\", nn.GELU()),\n",
        "            (\"c_proj\", nn.Linear(width * 4, width))\n",
        "        ]))\n",
        "        self.ln_2 = nn.LayerNorm(width)\n",
        "        self.attention_mask = attention_mask\n",
        "    \n",
        "    def attention(self, x):\n",
        "        attn_mask = self.attention_mask.to(dtype=x.dtype, device=x.device) if self.attention_mask is not None else None\n",
        "        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention(self.ln_1(x))\n",
        "        return self.mlp(self.ln_2(x))"
      ],
      "metadata": {
        "id": "K6Q-MaEIOsBf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, transformer_layers, transformer_width, transformer_heads, attention_mask=None):\n",
        "        super().__init__()\n",
        "        self.resblocks = nn.Sequential(*[AttentionBlock(transformer_width, transformer_heads, attention_mask) \n",
        "                                     for _ in range(transformer_layers)])\n",
        "        self.width = transformer_width\n",
        "        self.layers = transformer_layers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.resblocks(x)"
      ],
      "metadata": {
        "id": "CrTTX_BTXbiw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP(nn.Module):\n",
        "    def __init__(self, embed_dim, image_resolution, vision_layers, vision_width, \n",
        "                 vision_patch_size, context_length, vocab_size, transformer_width,\n",
        "                 transformer_heads, transformer_layers):\n",
        "        super().__init__()\n",
        "        vision_heads = vision_width // 64\n",
        "        self.context_length = context_length\n",
        "        self.visual = VisionTransformer(embed_dim, image_resolution, vision_layers, vision_width,\n",
        "                                                vision_heads, vision_patch_size)\n",
        "        self.transformer = Transformer(transformer_layers, transformer_width, transformer_heads)\n",
        "        #self.text_encoder = nn.Sequential(AttentionBlock(transformer_width, transformer_heads, attention_mask) \n",
        "        #                             for _ in range(transformer_layers))\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
        "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
        "        self.ln_final = nn.LayerNorm(transformer_width)\n",
        "\n",
        "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "\n",
        "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
        "        attn_std = self.transformer.width ** -0.5\n",
        "        fc_std = (2 * self.transformer.width) ** -0.5\n",
        "        for block in self.transformer.resblocks:\n",
        "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
        "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
        "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
        "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
        "\n",
        "        if self.text_projection is not None:\n",
        "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self.visual.conv1.weight.dtype\n",
        "    \n",
        "    def build_attention_mask(self):\n",
        "        mask = torch.empty(self.context_length, self.context_length)\n",
        "        mask.fill_(float(\"-inf\"))\n",
        "        mask.triu_(1) # Returns the upper triangular part of a matrix the other elements of the result tensor out are set to 0.\n",
        "        return mask\n",
        "    \n",
        "    def encode_text(self, text):\n",
        "        x = self.token_embedding(text).type(self.dtype)\n",
        "        x = x + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
        "        return x\n",
        "\n",
        "    def encode_image(self, x):\n",
        "        return self.visual(x)\n",
        "    \n",
        "    def forward(self, text, img):\n",
        "        vision_output = self.vision_encoder(img)\n",
        "        text_output = self.transformer(text)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        return logits_per_image, logits_per_text"
      ],
      "metadata": {
        "id": "CsmRxrHUW27l"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "LbadHNOb4pU9"
      },
      "outputs": [],
      "source": [
        "# https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/clip.py\n",
        "MODELS = {\n",
        "    \"ViT-B/32\": \"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"\n",
        "}\n",
        "\n",
        "def _convert_image_to_rgb(image):\n",
        "    return image.convert(\"RGB\")\n",
        "\n",
        "def load_params(param_path, device):\n",
        "    def _transform(n_px):\n",
        "        return Compose([\n",
        "            Resize(n_px, interpolation=BICUBIC),\n",
        "            CenterCrop(n_px),\n",
        "            _convert_image_to_rgb,\n",
        "            ToTensor(),\n",
        "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "        ])\n",
        "    \n",
        "    import urllib\n",
        "    if not os.path.exists(\"ViT-B-2.pt\"):\n",
        "        with urllib.request.urlopen(param_path) as source, open(\"ViT-B-2.pt\", \"wb\") as output:\n",
        "            with tqdm(total=int(source.headers.get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
        "                while True:\n",
        "                    buffer = source.read(8192)\n",
        "                    if not buffer:\n",
        "                        break\n",
        "\n",
        "                    output.write(buffer)\n",
        "                    loop.update(len(buffer))\n",
        "\n",
        "    with open(\"ViT-B-2.pt\", 'rb') as opened_file:\n",
        "        checkpoint = torch.load(opened_file, map_location=\"cpu\")\n",
        "        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
        "            state_dict = checkpoint['state_dict']\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "        if next(iter(state_dict.items()))[0].startswith('module'):\n",
        "            state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
        "\n",
        "\n",
        "    vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
        "    vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
        "    vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
        "    grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
        "    image_resolution = vision_patch_size * grid_size\n",
        "\n",
        "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
        "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
        "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
        "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
        "    transformer_heads = transformer_width // 64\n",
        "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n",
        "    \n",
        "    model = CLIP(embed_dim, image_resolution, vision_layers, vision_width, vision_patch_size,\n",
        "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers)\n",
        "\n",
        "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
        "        if key in state_dict:\n",
        "            del state_dict[key]\n",
        "    \n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.eval()\n",
        "    \n",
        "    return model, _transform(model.visual.input_resolution)\n",
        "\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = load_params(MODELS['ViT-B/32'], device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ftfy"
      ],
      "metadata": {
        "id": "3X7gfmLTspRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "pjLLbgIXtN1X",
        "outputId": "c2a7bffe-e7e1-4e76-ea2e-018827e0aeaf"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cecc7288-ee6c-451b-a611-6c76a8c91d29\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cecc7288-ee6c-451b-a611-6c76a8c91d29\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CLIP.png to CLIP.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJgXZRtItn16",
        "outputId": "69eb53da-815b-485d-8f16-1fd9818fc39a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bpe_simple_vocab_16e6.txt.gz  CLIP.png\tsample_data  ViT-B-2.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "def default_bpe():\n",
        "    return \"bpe_simple_vocab_16e6.txt.gz\"\n",
        "\n",
        "def bytes_to_unicode():\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "def get_pairs(word):\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, bpe_path=default_bpe()):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n",
        "\n",
        "_tokenizer = Tokenizer()\n",
        "\n",
        "def tokenize(texts, context_length: int = 77, truncate: bool = False):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
        "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
        "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
        "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)\n",
        "\n",
        "    for i, tokens in enumerate(all_tokens):\n",
        "        if len(tokens) > context_length:\n",
        "            if truncate:\n",
        "                tokens = tokens[:context_length]\n",
        "                tokens[-1] = eot_token\n",
        "            else:\n",
        "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
        "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "3MeoxUC4Qa6v"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "# Prepare the inputs\n",
        "#image, class_id = cifar100[3637]\n",
        "#image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "#text_inputs = torch.cat([tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0)\n",
        "text = tokenize([\"a diagram\", \"a dog\", \"a cat\"])\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "print(\"Label probs:\", similarity)\n",
        "\n",
        "# Print the result\n",
        "'''\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
        "'''"
      ],
      "metadata": {
        "id": "DhErs0aD1OB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}