{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC"
      ],
      "metadata": {
        "id": "-CXFjnBgZuZT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim, image_resolution, vision_layers, vision_width, vision_heads, \n",
        "                 vision_patch_size):\n",
        "        super().__init__()\n",
        "        self.input_resolution = image_resolution\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=vision_width, kernel_size=vision_patch_size,\n",
        "                               stride=vision_patch_size, bias=False)\n",
        "        scale = vision_width ** -0.5\n",
        "        self.class_embedding = nn.Parameter(scale * torch.randn(vision_width))\n",
        "        self.positional_embedding = nn.Parameter(scale * torch.randn((image_resolution // vision_patch_size) ** 2 + 1, vision_width))\n",
        "        self.ln_pre = nn.LayerNorm(vision_width)\n",
        "        self.transformer = Transformer(vision_layers, vision_width, vision_heads)\n",
        "        self.ln_post = nn.LayerNorm(vision_width)\n",
        "        self.proj = nn.Parameter(scale * torch.randn(vision_width, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = torch.cat([self.cls.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
        "        x = x + self.positional_embedding.to(x.dtype)\n",
        "        x = self.ln_pre(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.ln_post(x)\n",
        "        if self.proj:\n",
        "            x = x @ self.proj\n",
        "        return x"
      ],
      "metadata": {
        "id": "Af9YAmSGXmaE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, width, heads, attention_mask=None):\n",
        "        super().__init__()\n",
        "        # GPT-2 Attention block\n",
        "        # Masked MultiHeadAttnetion -> LayerNorm -> FF -> LayerNorm\n",
        "        self.attn = nn.MultiheadAttention(width, heads)\n",
        "        self.ln_1 = nn.LayerNorm(width)\n",
        "        self.mlp = nn.Sequential(OrderedDict([\n",
        "            (\"c_fc\", nn.Linear(width, width * 4)),\n",
        "            (\"gelu\", nn.GELU()),\n",
        "            (\"c_proj\", nn.Linear(width * 4, width))\n",
        "        ]))\n",
        "        self.ln_2 = nn.LayerNorm(width)\n",
        "        self.attention_mask = attention_mask\n",
        "    \n",
        "    def attention(self, x):\n",
        "        attn_mask = self.attention_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
        "        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention(self.ln1(x))\n",
        "        return self.mlp(self.ln2(x))"
      ],
      "metadata": {
        "id": "K6Q-MaEIOsBf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, transformer_layers, transformer_width, transformer_heads, attention_mask=None):\n",
        "        super().__init__()\n",
        "        self.resblocks = nn.Sequential(*[AttentionBlock(transformer_width, transformer_heads, attention_mask) \n",
        "                                     for _ in range(transformer_layers)])\n",
        "        self.width = transformer_width\n",
        "        self.layers = transformer_layers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.resblocks(x)"
      ],
      "metadata": {
        "id": "CrTTX_BTXbiw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP(nn.Module):\n",
        "    def __init__(self, embed_dim, image_resolution, vision_layers, vision_width, \n",
        "                 vision_patch_size, context_length, vocab_size, transformer_width,\n",
        "                 transformer_heads, transformer_layers):\n",
        "        super().__init__()\n",
        "        vision_heads = vision_width // 64\n",
        "        self.context_length = context_length\n",
        "        self.visual = VisionTransformer(embed_dim, image_resolution, vision_layers, vision_width,\n",
        "                                                vision_heads, vision_patch_size)\n",
        "        self.transformer = Transformer(transformer_layers, transformer_width, transformer_heads)\n",
        "        #self.text_encoder = nn.Sequential(AttentionBlock(transformer_width, transformer_heads, attention_mask) \n",
        "        #                             for _ in range(transformer_layers))\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
        "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
        "        self.ln_final = nn.LayerNorm(transformer_width)\n",
        "\n",
        "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "\n",
        "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
        "        attn_std = self.transformer.width ** -0.5\n",
        "        fc_std = (2 * self.transformer.width) ** -0.5\n",
        "        for block in self.transformer.resblocks:\n",
        "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
        "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
        "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
        "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
        "\n",
        "        if self.text_projection is not None:\n",
        "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
        "\n",
        "    def build_attention_mask(self):\n",
        "        mask = torch.empty(self.context_length, self.context_length)\n",
        "        mask.fill_(float(\"-inf\"))\n",
        "        mask.triu_(1) # Returns the upper triangular part of a matrix the other elements of the result tensor out are set to 0.\n",
        "        return mask\n",
        "    \n",
        "    def encode_text(self, x):\n",
        "        x = x + self.positional_embedding(x).type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = self.permute(1, 0, 2)\n",
        "        x = self.ln(x).type(self.dtype)\n",
        "        x = x[torch.arange(x.shape[0]), x.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "    def encode_image(self, x):\n",
        "        return self.visual(x)\n",
        "    \n",
        "    def forward(self, text, img):\n",
        "        vision_output = self.vision_encoder(img)\n",
        "        text_output = self.transformer(text)\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()\n",
        "\n",
        "        return logits_per_image, logits_per_text"
      ],
      "metadata": {
        "id": "CsmRxrHUW27l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LbadHNOb4pU9"
      },
      "outputs": [],
      "source": [
        "# https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/clip/clip.py\n",
        "MODELS = {\n",
        "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
        "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
        "    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",\n",
        "    \"ViT-B/32_jit\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
        "    \"ViT-B/32\": \"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"\n",
        "}\n",
        "\n",
        "def _convert_image_to_rgb(image):\n",
        "    return image.convert(\"RGB\")\n",
        "\n",
        "def load_params(param_path, device):\n",
        "    def _transform(n_px):\n",
        "        return Compose([\n",
        "            Resize(n_px, interpolation=BICUBIC),\n",
        "            CenterCrop(n_px),\n",
        "            _convert_image_to_rgb,\n",
        "            ToTensor(),\n",
        "            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "        ])\n",
        "    \n",
        "    import urllib\n",
        "    if not os.path.exists(\"ViT-B-2.pt\"):\n",
        "        with urllib.request.urlopen(param_path) as source, open(\"ViT-B-2.pt\", \"wb\") as output:\n",
        "            with tqdm(total=int(source.headers.get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
        "                while True:\n",
        "                    buffer = source.read(8192)\n",
        "                    if not buffer:\n",
        "                        break\n",
        "\n",
        "                    output.write(buffer)\n",
        "                    loop.update(len(buffer))\n",
        "\n",
        "    with open(\"ViT-B-2.pt\", 'rb') as opened_file:\n",
        "        checkpoint = torch.load(opened_file, map_location=\"cpu\")\n",
        "        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
        "            state_dict = checkpoint['state_dict']\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "        if next(iter(state_dict.items()))[0].startswith('module'):\n",
        "            state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
        "\n",
        "\n",
        "    vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
        "    vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
        "    vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
        "    grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
        "    image_resolution = vision_patch_size * grid_size\n",
        "\n",
        "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
        "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
        "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
        "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
        "    transformer_heads = transformer_width // 64\n",
        "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n",
        "    \n",
        "    model = CLIP(embed_dim, image_resolution, vision_layers, vision_width, vision_patch_size,\n",
        "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers)\n",
        "\n",
        "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
        "        if key in state_dict:\n",
        "            del state_dict[key]\n",
        "    \n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.eval()\n",
        "    \n",
        "    return model, _transform(model.visual.input_resolution)\n",
        "\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = load_params(MODELS['ViT-B/32'], device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "# Prepare the inputs\n",
        "image, class_id = cifar100[3637]\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "id": "DhErs0aD1OB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}