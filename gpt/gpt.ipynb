{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "mTeISoyWS8QT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ptb.train.txt\", 'r') as f:\n",
        "    lines = f.readlines()"
      ],
      "metadata": {
        "id": "XuDyN6flTGpC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens():\n",
        "  tokens = [list(line) for line in lines]\n",
        "  return tokens\n",
        "\n",
        "token = get_tokens()"
      ],
      "metadata": {
        "id": "Y623ZbO5bZkM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(tokens):\n",
        "  return [items for i in tokens for items in i]\n",
        "\n",
        "tokens = flatten(token)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9pmTC3OcMWz",
        "outputId": "6df98854-0989-4f09-e5d9-1dbfc906c565"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5101619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_char(tokens):\n",
        "  uniq_tokens = []\n",
        "  for i in tokens:\n",
        "    if i not in uniq_tokens:\n",
        "      uniq_tokens.append(i)\n",
        "  return uniq_tokens\n",
        "\n",
        "\n",
        "uniq_tokens = unique_char(tokens)\n",
        "print(len(uniq_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcc7hzbfcZe7",
        "outputId": "3f9e5a6a-d2ef-4845-9f2c-d065a0249f8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "for e, char in enumerate(uniq_tokens):\n",
        "  vocab[char] = e"
      ],
      "metadata": {
        "id": "id9STXYdcfSA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical = [vocab[char] for char in tokens]"
      ],
      "metadata": {
        "id": "mCXZxM13cf-n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 64\n",
        "num_samples = (len(numerical) - 1) // seq_length\n",
        "dataset = torch.tensor(numerical[:num_samples * seq_length]).reshape(num_samples, seq_length)\n",
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVQvaMI0cmMf",
        "outputId": "c60e075d-3158-4f3b-8435-79efe2c5f5a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([79712, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_batches = len(dataset) // batch_size\n",
        "train_iter = dataset[:num_batches * batch_size].reshape((num_batches, batch_size, seq_length))\n",
        "train_iter.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy4IQlLicoUm",
        "outputId": "114ea9f3-4b63-418d-ec3f-d4f2c4d3e808"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2491, 32, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.tensor(numerical[1:num_samples * seq_length + 1]).reshape(num_batches, batch_size, seq_length)\n",
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS7a70BBcqrt",
        "outputId": "b78c28de-2db9-43df-c3f3-2c3ee677e86e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2491, 32, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def textify(embedding):\n",
        "    result = \"\"\n",
        "    for idx in embedding:\n",
        "        result += uniq_tokens[int(idx)]\n",
        "    return result"
      ],
      "metadata": {
        "id": "1kVX8fJ4ctoE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(textify(train_iter[10, 3]))\n",
        "print(textify(labels[10, 3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En0pvuDbcwp5",
        "outputId": "5ea758b2-4971-46fb-e213-547b821bdea2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ter business appears to depend heavily on the creativity and <un\n",
            "er business appears to depend heavily on the creativity and <unk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.ln(self.dropout(y) + x)"
      ],
      "metadata": {
        "id": "rUCUP-8He1mL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, ffn_hiddens, d_model):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, ffn_hiddens)\n",
        "        self.act = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(ffn_hiddens, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.lin2(self.act(self.lin1(x)))"
      ],
      "metadata": {
        "id": "cKY0rMwWe-Us"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.value = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.output = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = 1 / math.sqrt(self.d_k)\n",
        "        \n",
        "    def forward(self, k, q, v, mask=None):\n",
        "        batch_size, seq_len = q.shape[0], q.shape[1]\n",
        "        q = self.query(q)\n",
        "        k = self.key(k)\n",
        "        v = self.value(v)\n",
        "        \n",
        "        Q = q.view(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        K = k.view(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        V = v.view(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "    \n",
        "        scores = (Q @ K.permute(0, 1, 3, 2)) * self.scale\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        x = self.dropout(attn) @ V\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, -1, self.d_model)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ny3Npa6LTI2D"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ffn_hiddens, dropout):\n",
        "        super().__init__()\n",
        "        # MultiheadAttention -> AddNorm -> FFN -> AddNorm\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        self.ffn = FeedForward(ffn_hiddens, d_model)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.addnorm1(x, self.attention(x, x, x, mask=mask))\n",
        "        x = self.addnorm2(x, self.ffn(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "PxO-7bjta-YE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len = config.seq_len\n",
        "        self.d_model = config.d_model\n",
        "        self.embedding = nn.Embedding(config.vocab_size, self.d_model)\n",
        "        self.pos_encoding = nn.Embedding(config.seq_len, config.d_model)\n",
        "        self.dec_blocks = nn.Sequential(*[DecoderBlock(self.d_model, config.num_heads, config.ffn_hiddens, \n",
        "                                                       config.dropout_prob) for _ in range(config.num_blocks)])\n",
        "        self.lin_head = nn.Linear(self.d_model, self.d_model)\n",
        "        self.mask = None\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def subsequent_mask(self, seq_len):\n",
        "        # Mask data from future time steps\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(torch.bool).view(1, 1, seq_len, seq_len)\n",
        "        return mask\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn\n",
        "                if pn.endswith('bias'):\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, x, target=None):\n",
        "        if self.mask is None:\n",
        "            self.mask = self.subsequent_mask(self.seq_len)\n",
        "        pos = torch.arange(0, x.shape[1], dtype=torch.long).unsqueeze(0)\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(pos) + x\n",
        "        for blk in self.dec_blocks:\n",
        "            x = blk(x, mask=self.mask)\n",
        "        x = self.lin_head(x)\n",
        "        loss = None\n",
        "        if target is not None:\n",
        "            loss = F.cross_entropy(x.view(-1, x.size(-1)), target.view(-1), ignore_index=-1)\n",
        "        return x, loss"
      ],
      "metadata": {
        "id": "u0dJZRorbJfS"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    d_model: int = 512\n",
        "    vocab_size: int = 50\n",
        "    seq_len: int = 64\n",
        "    dropout_prob: float = 0.1\n",
        "    ffn_hiddens: int = 48\n",
        "    num_blocks: int = 6\n",
        "    num_heads: int = 8\n",
        "    weight_decay: float = 0.1\n",
        "    grad_norm_clip: float = 1.0\n",
        "    num_epochs: int = 10\n",
        "    learning_rate: float = 3e-4\n",
        "    betas: tuple = (0.9, 0.95)"
      ],
      "metadata": {
        "id": "knnd3S07bONi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig()\n",
        "net = GPT(config)\n",
        "optimizer = net.configure_optimizers(config)\n",
        "\n",
        "net.train()\n",
        "for epoch in range(config.num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for x, y in tqdm(zip(train_iter, labels)):\n",
        "        optimizer.zero_grad()\n",
        "        y_hat, loss = net(x, y)\n",
        "        loss.backward()\n",
        "        epoch_loss += loss\n",
        "        print(loss)\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), config.grad_norm_clip)\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(epoch_loss)"
      ],
      "metadata": {
        "id": "AD4IHEwrbSNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}