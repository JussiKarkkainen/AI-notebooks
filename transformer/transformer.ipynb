{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pvrT2tCHyVx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Tuple\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "id": "E-As75FyIYx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> torch.Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "metadata": {
        "id": "kZbD5m0GIveo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)"
      ],
      "metadata": {
        "id": "0bppTT-OJ5bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "oOkRRaJDKu_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data: torch.Tensor, bsz: int) -> torch.Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "vHCEv1zZKh2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 35\n",
        "def get_batch(source: torch.Tensor, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n"
      ],
      "metadata": {
        "id": "cZLWFLHuK-N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, target = get_batch(train_data, 0)\n",
        "print(data.shape, target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ5RsjmILoYL",
        "outputId": "c6a67f13-59bd-4cbb-dc77-029bf3b2ac80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([35, 20]) torch.Size([700])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Start of Transformer ---"
      ],
      "metadata": {
        "id": "edqdbDekMQnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pass\n",
        "    def forward(self, X):\n",
        "        pass"
      ],
      "metadata": {
        "id": "QZfgdasApp9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pass\n",
        "    def forward(self, X):\n",
        "        pass"
      ],
      "metadata": {
        "id": "V5F23N2-ozY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, norm_shape, dropout):\n",
        "        super().__init__()\n",
        "        self.layer_norm = LayerNorm(norm_shape)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, X, Y):\n",
        "        return self.layer_norm(self.dropout(Y) + X)"
      ],
      "metadata": {
        "id": "IaavRBABOnxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout, num_heads):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        d_dim = q.shape[-1]\n",
        "        k_t = k.transpose(2, 3)\n",
        "        score = q.matmul(k_t) / math.sqrt(d_dim)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask, -e)\n",
        "        out = nn.softmax(score)\n",
        "        v_matmul = out.mamtul(v)\n",
        "        return v_matmul"
      ],
      "metadata": {
        "id": "uZHOqFEQpeUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, d_model):\n",
        "        super().__init__()\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_out = nn.Linear(d_model, d_model)\n",
        "        self.attention = ScaledDotProductAttention(dropout, num_heads)\n",
        "        \n",
        "    def forward(self, q, k, v mask=None):\n",
        "        # Dot product with weights + split between attention heads \n",
        "        q = self.split(self.w_q(q))\n",
        "        k = self.split(self.w_k(k))\n",
        "        v = self.split(self.w_v(v))\n",
        "\n",
        "        out = self.attention(q, k, v, mask=mask)\n",
        "        out = self.concat(out)\n",
        "        return self.w_out(out)"
      ],
      "metadata": {
        "id": "Mrz6v2ATpEOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, num_heads, ffn_in, ffn_hidden, ffn_output):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads, d_model)\n",
        "        self.addnorm = AddNorm(ffn_in, ffn_hidden)\n",
        "        self.ffn = nn.Sequential(nn.Linear(ffn_in, ffn_hidden),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Linear(ffn_hidden, ffn_output))\n",
        "        \n",
        "    def forward(self, X):\n",
        "        # 2. Attention + AddNorm\n",
        "        Y = self.addnorm(X, self.attention(X, X, X, mask))\n",
        "        # 4. PositionWise FFN\n",
        "        ffn_out = self.ffn(Y)\n",
        "        # 5. AddNorm\n",
        "        return self.addnorm(Y, ffn_out)"
      ],
      "metadata": {
        "id": "xjbZcy3gMyeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, num_blocks, num_heads, \n",
        "                 num_hiddens, ffn_in, ffn_hidden):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.embedding = nn.Embedding()\n",
        "        self.blocks = nn.Sequential()\n",
        "        for i in range(num_blocks):\n",
        "            self.blocks.add_module(\"block\"+str(i), EncoderBlock(ffn_in, ffn_hidden, ffn_output))\n",
        "   \n",
        "    def forward(self, X):\n",
        "        # 1. Pos-Encoding + Embedding\n",
        "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)\n",
        "        for blk in self.blocks:\n",
        "            X = blk(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "wnRGNAZyMVfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, num_hiddens, ffn_hiddens, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.attention1 = MultiHeadAttention(num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
        "        self.attention2 = MultiHeadAttention(num_heads, dropout)\n",
        "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
        "        self.ffn = PositionWiseFfn()\n",
        "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
        "\n",
        "    def forward(self, dec, enc, trg_mask):\n",
        "        # 1. Masked Self-Attention\n",
        "        _x = dec\n",
        "        x = self.attention1(dec, dec, dec, mask)\n",
        "        # 2. AddNorm\n",
        "        x = self.norm1(x, _x)\n",
        "        # 3. Encoder-Decoder Attention\n",
        "        _x = x\n",
        "        x = self.attention2(x, enc, enc, mask)\n",
        "        # 4. AddNorm\n",
        "        x = self.norm2(x, _x)\n",
        "        # 5. PositionWise FFN\n",
        "        _x = x\n",
        "        x = self.ffn(x)\n",
        "        # 6. AddNorm\n",
        "        out = self.addnorm3(x, _x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Xhd0FbQkNPY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential()\n",
        "        for i in range(num_blocks):\n",
        "            self.blocks.add_module(\"block\"+str(i), DecoderBlock(ffn_in, ffn_hidden, ffn_output))\n",
        "    def forward(self, X):\n",
        "        pass"
      ],
      "metadata": {
        "id": "yREON-LOMn2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_hiddens, ffn_hiddens, num_heads, num_blks, dropout)\n",
        "        self.decoder = Decoder(num_hiddens, ffn_hiddens, num_heads, num_blks, dropout)\n",
        "\n",
        "    def forward(self, X):\n",
        "        memory = self.encoder(src, mas)\n",
        "        out = self.decoder()"
      ],
      "metadata": {
        "id": "mxgPtvoSNwJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Transformer()\n",
        "criterion = nn.CrossEntropyloss()\n",
        "optimizer = torch.optim.Adam(net.Parameters())\n",
        "\n",
        "num_epochs = 100\n",
        "for i in range(num_epochs).\n",
        "fir "
      ],
      "metadata": {
        "id": "GLhWguZGMcuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}