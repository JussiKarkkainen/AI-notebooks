{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--O8l3gH7KR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"en_train.txt\", 'r') as f:\n",
        "    en_lines = f.readlines()\n",
        "with open(\"fr_train.txt\", 'r') as F:\n",
        "    fr_lines = F.readlines()"
      ],
      "metadata": {
        "id": "i9CyV2YoY2X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(lines):\n",
        "    for i, line in enumerate(lines):\n",
        "        lines[i] = line.replace(' ', '')\n",
        "        lines[i] = line.strip()\n",
        "    tokens = [list(line) for line in lines]\n",
        "    return tokens\n",
        "\n",
        "en_token = get_tokens(en_lines)\n",
        "fr_token = get_tokens(fr_lines)\n",
        "en_token[:10]"
      ],
      "metadata": {
        "id": "kqXis5RrZMoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da167e82-b470-4b5b-aa21-754a82e25093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['G', 'o', '.'],\n",
              " ['H', 'i', '.'],\n",
              " ['H', 'i', '.'],\n",
              " ['R', 'u', 'n', '!'],\n",
              " ['R', 'u', 'n', '!'],\n",
              " ['W', 'h', 'o', '?'],\n",
              " ['W', 'o', 'w', '!'],\n",
              " ['F', 'i', 'r', 'e', '!'],\n",
              " ['H', 'e', 'l', 'p', '!'],\n",
              " ['J', 'u', 'm', 'p', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(tokens):\n",
        "    return [items for i in tokens for items in i]\n",
        "\n",
        "en_tokens_flat = flatten(en_token)\n",
        "fr_tokens_flat = flatten(fr_token)\n",
        "print(len(en_tokens_flat))\n",
        "print(len(fr_tokens_flat))"
      ],
      "metadata": {
        "id": "7luSCs9BcSnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d84d31d-a6f5-4e8e-e687-84592bd495dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3529268\n",
            "4243940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_char(tokens):\n",
        "    uniq_tokens = []\n",
        "    for i in tokens:\n",
        "        if i not in uniq_tokens:\n",
        "            uniq_tokens.append(i)\n",
        "    return uniq_tokens\n",
        "\n",
        "uniq_en_tokens = unique_char(en_tokens_flat)\n",
        "uniq_fr_tokens = unique_char(fr_tokens_flat)\n",
        "print(len(uniq_en_tokens))\n",
        "print(len(uniq_fr_tokens))\n",
        "print(uniq_en_tokens)"
      ],
      "metadata": {
        "id": "IoQJ0362cXDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743fb9c5-bbb8-47f4-d3d6-a83f252bb996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84\n",
            "106\n",
            "['G', 'o', '.', 'H', 'i', 'R', 'u', 'n', '!', 'W', 'h', '?', 'w', 'F', 'r', 'e', 'l', 'p', 'J', 'm', 'S', 't', 'a', ' ', 'I', 's', 'y', 'O', 'A', 'c', 'k', 'C', 'g', 'f', 'd', \"'\", '1', '9', 'K', 'L', 'N', 'T', 'B', 'D', 'b', 'q', 'z', 'v', 'M', ',', 'P', 'Y', 'x', 'j', 'U', 'E', '$', '5', '3', ':', '0', '8', 'V', '7', '&', '%', '-', '2', 'Q', '6', '4', '\"', 'X', 'Z', 'é', '’', '€', '/', 'ç', '‘', 'а', '\\xad', '–', 'ö']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(unique_tokens, trg=False):\n",
        "    vocab = {}\n",
        "    vocab[\"<PAD>\"] = 0\n",
        "    if trg:\n",
        "        vocab[\"<START>\"] = 1\n",
        "        vocab[\"<END>\"] = 2\n",
        "    for e, char in enumerate(unique_tokens):\n",
        "        vocab[char] = (e + 1) if not trg else (e + 3)\n",
        "    return vocab\n",
        "\n",
        "en_vocab = build_vocab(uniq_en_tokens)\n",
        "fr_vocab = build_vocab(uniq_fr_tokens, trg=True)\n",
        "print(fr_vocab)"
      ],
      "metadata": {
        "id": "1Wjh-c9scbJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1bbe4f8-ca45-44da-f0f4-3c73391744a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<PAD>': 0, '<START>': 1, '<END>': 2, 'V': 3, 'a': 4, ' ': 5, '!': 6, 'S': 7, 'l': 8, 'u': 9, 't': 10, '.': 11, 'C': 12, 'o': 13, 'r': 14, 's': 15, '\\u202f': 16, 'e': 17, 'z': 18, 'Q': 19, 'i': 20, '?': 21, 'Ç': 22, 'A': 23, 'f': 24, 'À': 25, \"'\": 26, 'd': 27, 'p': 28, 'ê': 29, '-': 30, 'n': 31, 'P': 32, 'v': 33, 'B': 34, 'j': 35, 'J': 36, 'c': 37, 'm': 38, 'y': 39, 'g': 40, 'é': 41, '’': 42, 'O': 43, 'h': 44, 'q': 45, 'M': 46, 'T': 47, 'L': 48, 'è': 49, ',': 50, 'b': 51, '1': 52, '9': 53, 'É': 54, 'I': 55, 'E': 56, 'ç': 57, 'x': 58, 'H': 59, 'N': 60, 'â': 61, 'D': 62, 'à': 63, 'F': 64, 'R': 65, 'G': 66, 'î': 67, 'û': 68, '\\u2009': 69, 'U': 70, 'ô': 71, 'k': 72, 'K': 73, '8': 74, '3': 75, '0': 76, 'Ê': 77, ':': 78, '«': 79, '»': 80, 'ù': 81, 'œ': 82, 'ï': 83, '5': 84, 'Y': 85, '&': 86, '%': 87, '(': 88, ')': 89, '2': 90, '$': 91, 'ë': 92, 'w': 93, '6': 94, '‘': 95, '4': 96, 'Ô': 97, '7': 98, '\"': 99, 'X': 100, 'W': 101, 'Z': 102, '\\u200b': 103, 'С': 104, '+': 105, '‽': 106, '…': 107, 'ö': 108}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_numerical(tokens, vocab):\n",
        "    new_lines = []\n",
        "    for line in tokens:\n",
        "        new_line = []\n",
        "        for char in line:\n",
        "            new_line.append(vocab[char])\n",
        "        new_lines.append(new_line)\n",
        "    return new_lines\n",
        "\n",
        "\n",
        "en_numerical = build_numerical(en_token, en_vocab)\n",
        "fr_numerical = build_numerical(fr_token, fr_vocab)\n",
        "en_numerical[:10]"
      ],
      "metadata": {
        "id": "buvkOpsncjEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3b7ddf-61c7-44ca-9dfb-8cc166469065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3],\n",
              " [4, 5, 3],\n",
              " [4, 5, 3],\n",
              " [6, 7, 8, 9],\n",
              " [6, 7, 8, 9],\n",
              " [10, 11, 2, 12],\n",
              " [10, 2, 13, 9],\n",
              " [14, 5, 15, 16, 9],\n",
              " [4, 16, 17, 18, 9],\n",
              " [19, 7, 20, 18, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence(numerical, vocab):\n",
        "    def _get_max_len(numerical):\n",
        "        max = 0\n",
        "        for i in numerical:\n",
        "            length = len(i)\n",
        "            if length > max:\n",
        "                max = length\n",
        "        return max\n",
        "    pad_token = \"<PAD>\"\n",
        "    max_len = _get_max_len(numerical)\n",
        "    for i in numerical:\n",
        "        while len(i) < max_len:\n",
        "            i.append(vocab[pad_token])\n",
        "    return numerical, vocab\n",
        "\n",
        "padded_en_numerical, en_vocab = pad_sequence(en_numerical, en_vocab)\n",
        "padded_fr_numerical, fr_vocab = pad_sequence(fr_numerical, fr_vocab)"
      ],
      "metadata": {
        "id": "l3J7IMjMK3rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = torch.tensor(src_data)\n",
        "        self.tgt_data = torch.tensor(tgt_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.tgt_data[idx]"
      ],
      "metadata": {
        "id": "o2lzXPjg4dBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----- START OF TRANSFORMER -----"
      ],
      "metadata": {
        "id": "a4_XM-c9Ed54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TranslationDataset(en_numerical, fr_numerical)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "cy8YLBn35gKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, sequence_len, d_model, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.sequence_len = sequence_len\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.register_buffer(\"positional_encoding\", self.get_pos_encoding(d_model, sequence_len), False)\n",
        "\n",
        "    def get_pos_encoding(self, d_model, max_len):\n",
        "        encodings = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "        div_term = torch.exp(two_i * -(math.log(10000.0) / d_model))\n",
        "        encodings[:, 0::2] = torch.sin(position * div_term)\n",
        "        encodings[:, 1::2] = torch.cos(position * div_term)\n",
        "        encodings = encodings.unsqueeze(1).requires_grad_(False)\n",
        "        return encodings\n",
        "\n",
        "    def forward(self, x):\n",
        "        pe = self.positional_encoding[:x.shape[0]].detach().requires_grad_(False)\n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VpP2HyzNX3qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.ln(self.dropout(y) + x)"
      ],
      "metadata": {
        "id": "wJH0y_aJajKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, ffn_hiddens, d_model):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, ffn_hiddens)\n",
        "        self.act = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(ffn_hiddens, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.lin2(self.act(self.lin1(x)))"
      ],
      "metadata": {
        "id": "rb-l84rxaYNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.value = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.output = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = 1 / math.sqrt(self.d_k)\n",
        "        \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.shape[0]\n",
        "        q = self.query(q)\n",
        "        k = self.key(k)\n",
        "        v = self.value(v)\n",
        "        \n",
        "        Q = q.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        K = k.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        V = v.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = Q @ K.permute(0, 1, 3, 2)\n",
        "        scores *= self.scale\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        x = self.dropout(attn) @ V\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, -1, self.d_model)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1LXCEbbCmgDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ffn_hiddens, dropout):\n",
        "        super().__init__()\n",
        "        # MultiheadAttention -> AddNorm -> FFN -> AddNorm\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        self.ffn = FeedForward(ffn_hiddens, d_model)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.addnorm1(x, self.attention(x, x, x, mask=src_mask))\n",
        "        x = self.addnorm2(x, self.ffn(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "XLtJ0hRWctF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, sequence_len, num_heads, num_blocks, ffn_hiddens, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(sequence_len, d_model, dropout_prob)\n",
        "        self.enc_blocks = nn.Sequential(*[EncoderBlock(d_model, num_heads, ffn_hiddens, dropout_prob)\n",
        "                                        for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # (batch_size, seq_len) (32, 64)\n",
        "        x = self.pos_encoding(self.embedding(x) * math.sqrt(self.d_model))\n",
        "        # (batch_size, seq_len, d_model) (32, 64, 512)\n",
        "        for blk in self.enc_blocks:\n",
        "            x = blk(x, src_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vhvPxGJNUJKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout, ffn_hiddens):\n",
        "        super().__init__()\n",
        "        # Masked MHA -> AddNorm -> EncoderDecoder MHA -> AddNorm -> FFN -> AddNorm\n",
        "        self.mask_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        \n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "\n",
        "        self.ffn = FeedForward(ffn_hiddens, d_model)\n",
        "        self.addnorm3 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, dec, enc, trg_mask, src_mask):\n",
        "        attention = self.mask_attention(dec, dec, dec, mask=trg_mask)\n",
        "        _x = self.addnorm1(dec, attention)\n",
        "        x = self.enc_dec_attn(_x, enc, enc, mask=src_mask)\n",
        "        x = self.addnorm2(_x, x)\n",
        "        _x = self.ffn(x)\n",
        "        x = self.addnorm3(_x, x)\n",
        "        return x "
      ],
      "metadata": {
        "id": "BD4Xr41Scgyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, ffn_hiddens, num_blocks, num_heads, dropout, sequence_len):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(sequence_len, d_model, dropout)\n",
        "        self.dec_blocks = nn.Sequential(*[DecoderBlock(d_model, num_heads, dropout, ffn_hiddens)\n",
        "                                        for _ in range(num_blocks)])\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, trg, enc_out, trg_mask, src_mask):\n",
        "        x = self.pos_encoding(self.embedding(trg) * math.sqrt(self.d_model))\n",
        "        for blk in self.dec_blocks:\n",
        "            x = blk(x, enc_out, trg_mask, src_mask)\n",
        "        x = self.lin(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rlYO81FGc_U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(config.d_model, config.enc_vocab_size, config.sequence_len,\n",
        "                                         config.num_heads, config.num_blocks, config.ffn_hiddens, \n",
        "                                         config.dropout_prob)\n",
        "        self.decoder = TransformerDecoder(config.dec_vocab_size, config.d_model, config.ffn_hiddens, config.num_blocks, \n",
        "                                         config.num_heads, config.dropout_prob, config.sequence_len)\n",
        "\n",
        "    def encode(self, x, src_mask):\n",
        "        return self.encoder(x, src_mask)\n",
        "\n",
        "    def decode(self, trg, enc_out, trg_mask, src_mask):\n",
        "        return self.decoder(trg, enc_out, trg_mask, src_mask)\n",
        "    \n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != 0).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_subsequent_mask = torch.tril(torch.ones(trg_len, trg_len)).bool()\n",
        "        trg_mask = trg_pad_mask & trg_subsequent_mask\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_out = self.encode(src, src_mask)\n",
        "        x = self.decode(trg, enc_out, trg_mask, src_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mxnBbqSeEj1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerConfig:\n",
        "    d_model: int = 512\n",
        "    enc_vocab_size: int = 84\n",
        "    dec_vocab_size: int = 106\n",
        "    sequence_len: int = 83\n",
        "    dropout_prob: float = 0.1\n",
        "    ffn_hiddens: int = 2048\n",
        "    num_blocks: int = 6\n",
        "    num_heads: int = 8\n",
        "    "
      ],
      "metadata": {
        "id": "0Lh2nqkiV9Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "config = TransformerConfig()\n",
        "net = Transformer(config)\n",
        "lossfn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(net.parameters(), 3e-4)\n",
        "\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for x, y in dataloader:\n",
        "        # (batch_size, seq_len)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = net(x, y[:, :-1])\n",
        "        y_hat = y_hat.contiguous().view(-1, y_hat.shape[-1])\n",
        "        y = y[:, 1:].contiguous().view(-1)\n",
        "        loss = lossfn(y_hat, y)\n",
        "        print(f\"Loss is: {loss}\")\n",
        "        epoch_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Loss on epoch {epoch} was {epoch_loss.item}\")\n"
      ],
      "metadata": {
        "id": "R8KGUahaEzxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}