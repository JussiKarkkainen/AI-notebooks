{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--O8l3gH7KR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "VVwW7O7vAyiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75941b0f-d646-4b0a-fdde-1d34645a35c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptb.train.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ptb.train.txt\", 'r') as f:\n",
        "    lines = f.readlines()"
      ],
      "metadata": {
        "id": "aNB8WOGeA5fc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens():\n",
        "  tokens = [list(line) for line in lines]\n",
        "  return tokens\n",
        "\n",
        "token = get_tokens()"
      ],
      "metadata": {
        "id": "CqbJYxPtB2jH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(tokens):\n",
        "  return [items for i in tokens for items in i]\n",
        "\n",
        "tokens = flatten(token)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "EJWv6fJKB6n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966c694e-0d79-4037-bbfa-51ff107c08c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5101619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_char(tokens):\n",
        "  uniq_tokens = []\n",
        "  for i in tokens:\n",
        "    if i not in uniq_tokens:\n",
        "      uniq_tokens.append(i)\n",
        "  return uniq_tokens\n",
        "\n",
        "\n",
        "uniq_tokens = unique_char(tokens)\n",
        "print(len(uniq_tokens))"
      ],
      "metadata": {
        "id": "jtb-QrVDCAo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b1a149-2357-412c-d3b0-2fc83148075a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "for e, char in enumerate(uniq_tokens):\n",
        "  vocab[char] = e"
      ],
      "metadata": {
        "id": "ZTxEShZaCDlV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical = [vocab[char] for char in tokens]"
      ],
      "metadata": {
        "id": "f_7QAU_jCEnk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 64\n",
        "num_samples = (len(numerical) - 1) // seq_length\n",
        "dataset = torch.tensor(numerical[:num_samples * seq_length]).reshape(num_samples, seq_length)\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "SE72A6KICHwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e1ef95-d42c-4a2e-f751-6d70d1c7dcb1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([79712, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_batches = len(dataset) // batch_size\n",
        "train_iter = dataset[:num_batches * batch_size].reshape((num_batches, batch_size, seq_length))\n",
        "train_iter.shape"
      ],
      "metadata": {
        "id": "9O6eWUpMDyoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b053ff-ba54-448d-8e5e-af5769c07f75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2491, 32, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.tensor(numerical[1:num_samples * seq_length + 1]).reshape(num_batches, batch_size, seq_length)\n",
        "labels.shape"
      ],
      "metadata": {
        "id": "qfvnHH4bFAJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5218e192-ef00-4fe4-c0a7-635d828e6866"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2491, 32, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def textify(embedding):\n",
        "    result = \"\"\n",
        "    for idx in embedding:\n",
        "        result += uniq_tokens[int(idx)]\n",
        "    return result"
      ],
      "metadata": {
        "id": "w3u7ZxRAETfA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(textify(train_iter[10, 3]))\n",
        "print(textify(labels[10, 3]))"
      ],
      "metadata": {
        "id": "OoJED4FcEZDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f81a299-3ff7-4b65-9d5c-be300db3c5db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ter business appears to depend heavily on the creativity and <un\n",
            "er business appears to depend heavily on the creativity and <unk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----- START OF TRANSFORMER -----"
      ],
      "metadata": {
        "id": "a4_XM-c9Ed54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, sequence_len, d_model, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.sequence_len = sequence_len\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.register_buffer(\"positional_encoding\", self.get_pos_encoding(d_model, sequence_len), False)\n",
        "\n",
        "    def get_pos_encoding(self, d_model, max_len):\n",
        "        encodings = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "        div_term = torch.exp(two_i * -(math.log(10000.0) / d_model))\n",
        "        encodings[:, 0::2] = torch.sin(position * div_term)\n",
        "        encodings[:, 1::2] = torch.cos(position * div_term)\n",
        "        encodings = encodings.unsqueeze(1).requires_grad_(False)\n",
        "        return encodings\n",
        "\n",
        "    def forward(self, x):\n",
        "        pe = self.positional_encoding[:x.shape[0]].detach().requires_grad_(False)\n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VpP2HyzNX3qN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.ln(self.dropout(y) + x)"
      ],
      "metadata": {
        "id": "wJH0y_aJajKl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, ffn_hiddens, d_model):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, ffn_hiddens)\n",
        "        self.act = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(ffn_hiddens, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.lin2(self.act(self.lin1(x)))"
      ],
      "metadata": {
        "id": "rb-l84rxaYNf"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.value = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.output = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = 1 / math.sqrt(self.d_k)\n",
        "        \n",
        "    def forward(self, k, q, v, mask=None):\n",
        "        batch_size = q.shape[0]\n",
        "        q = self.query(q)\n",
        "        k = self.key(k)\n",
        "        v = self.value(v)\n",
        "        \n",
        "        Q = q.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        K = k.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        V = v.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = Q @ K.permute(0, 1, 3, 2)\n",
        "        scores *= self.scale\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        x = self.dropout(attn) @ V\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, -1, self.d_model)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1LXCEbbCmgDV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ffn_hiddens, dropout):\n",
        "        super().__init__()\n",
        "        # MultiheadAttention -> AddNorm -> FFN -> AddNorm\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        self.ffn = FeedForward(ffn_hiddens, d_model)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.addnorm1(x, self.attention(x, x, x))\n",
        "        x = self.addnorm2(x, self.ffn(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "XLtJ0hRWctF1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, sequence_len, num_heads, num_blocks, ffn_hiddens, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(sequence_len, d_model, dropout_prob)\n",
        "        self.enc_blocks = nn.Sequential(*[EncoderBlock(d_model, num_heads, ffn_hiddens, dropout_prob)\n",
        "                                        for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, seq_len) (32, 64)\n",
        "        x = self.pos_encoding(self.embedding(x) * math.sqrt(self.d_model))\n",
        "        # (batch_size, seq_len, d_model) (32, 64, 512)\n",
        "        for blk in self.enc_blocks:\n",
        "            x = blk(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vhvPxGJNUJKV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout, ffn_hiddens):\n",
        "        super().__init__()\n",
        "        self.mask_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        \n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "\n",
        "        self.ffn = FeedForward(ffn_hiddens, d_model)\n",
        "        self.addnorm3 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, dec, enc, trg_mask, src_mask):\n",
        "        attention = self.mask_attention(dec, dec, dec, mask=trg_mask)\n",
        "        _x = self.addnorm1(dec, attention)\n",
        "        x = enc_dec_attn(_x, enc, enc, mask=src_mask)\n",
        "        x = self.addnorm2(_x, x)\n",
        "        _x = self.ffn(x)\n",
        "        x = self.addnorm3(_x, x)\n",
        "        return x "
      ],
      "metadata": {
        "id": "BD4Xr41Scgyd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_blocks, num_heads, dropout, sequence_len):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(sequence_len, d_model, dropout)\n",
        "        self.dec_blocks = nn.Sequential(*[DecoderBlock(d_model, num_heads, dropout)\n",
        "                                        for _ in range(num_blocks)])\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pos_encoding(self.embedding(x) * math.sqrt(self.d_model))\n",
        "        for blk in self.dec_blocks:\n",
        "            x = blk(x)\n",
        "        x = torch.softmax(self.lin(x))\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "rlYO81FGc_U_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(config.d_model, config.vocab_size, config.sequence_len,\n",
        "                                         config.num_heads, config.num_blocks, config.ffn_hiddens, \n",
        "                                         config.enc_dropout_prob)\n",
        "        self.decoder = TransformerDecoder(config.vocab_size, config.d_model, config.num_blocks, \n",
        "                                         config.num_heads, config.dec_dropout_prob, config.sequence_len)\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        raise Exception(\"decode\")\n",
        "        x = self.decode(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mxnBbqSeEj1S"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerConfig:\n",
        "    d_model: int = 512\n",
        "    vocab_size: int = 50\n",
        "    sequence_len: int = 64\n",
        "    enc_dropout_prob: float = 0.5\n",
        "    dec_dropout_prob: float = 0.5\n",
        "    dropout_prob: float = 0.2\n",
        "    ffn_hiddens: int = 48\n",
        "    num_blocks: int = 6\n",
        "    num_heads: int = 8\n",
        "    "
      ],
      "metadata": {
        "id": "0Lh2nqkiV9Z5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "config = TransformerConfig()\n",
        "net = Transformer(config)\n",
        "lossfn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), 1e-3)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x, y in zip(train_iter, labels):\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = net(x)\n",
        "        raise Exception(\"here\")\n",
        "        loss = lossfn(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        "
      ],
      "metadata": {
        "id": "R8KGUahaEzxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}