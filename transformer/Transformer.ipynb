{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--O8l3gH7KR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = files.upload()"
      ],
      "metadata": {
        "id": "VVwW7O7vAyiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"ptb.train.txt\", 'r') as f:\n",
        "    lines = f.readlines()"
      ],
      "metadata": {
        "id": "aNB8WOGeA5fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens():\n",
        "  tokens = [list(line) for line in lines]\n",
        "  return tokens\n",
        "\n",
        "token = get_tokens()"
      ],
      "metadata": {
        "id": "CqbJYxPtB2jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(tokens):\n",
        "  return [items for i in tokens for items in i]\n",
        "\n",
        "tokens = flatten(token)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "EJWv6fJKB6n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_char(tokens):\n",
        "  uniq_tokens = []\n",
        "  for i in tokens:\n",
        "    if i not in uniq_tokens:\n",
        "      uniq_tokens.append(i)\n",
        "  return uniq_tokens\n",
        "\n",
        "\n",
        "uniq_tokens = unique_char(tokens)\n",
        "print(len(uniq_tokens))"
      ],
      "metadata": {
        "id": "jtb-QrVDCAo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "for e, char in enumerate(uniq_tokens):\n",
        "  vocab[char] = e"
      ],
      "metadata": {
        "id": "ZTxEShZaCDlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical = [vocab[char] for char in tokens]"
      ],
      "metadata": {
        "id": "f_7QAU_jCEnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 64\n",
        "num_samples = (len(wiki_numerical) - 1) // seq_length\n",
        "dataset = wiki_numerical[:num_samples * seq_length].reshape(num_samples, seq_length)\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "SE72A6KICHwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_batches = len(dataset) // batch_size\n",
        "train_iter = dataset[:num_batches * batch_size].reshape((num_batches, batch_size, seq_length))\n",
        "train_iter.shape"
      ],
      "metadata": {
        "id": "9O6eWUpMDyoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = numerical[1:num_samples * seq_length + 1]).reshape(num_batches, batch_size, seq_length)\n",
        "labels.shape"
      ],
      "metadata": {
        "id": "qfvnHH4bFAJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def textify(embedding):\n",
        "    result = \"\"\n",
        "    indices = torch.argmax(embedding, axis=1)\n",
        "    for idx in indices:\n",
        "        result += uniq_tokens[int(idx)]\n",
        "    return result"
      ],
      "metadata": {
        "id": "w3u7ZxRAETfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(textify(train_iter[10, :, 3]))\n",
        "print(textify(labels[10, :, 3]))"
      ],
      "metadata": {
        "id": "OoJED4FcEZDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----- START OF TRANSFORMER -----"
      ],
      "metadata": {
        "id": "a4_XM-c9Ed54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, sequence_len, d_model, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.sequence_len = sequence_len\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.register_buffer(\"positional_encoding\", self.get_pos_encoding(d_model, sequence_len), False)\n",
        "\n",
        "    def get_pos_encoding(self, d_model, max_len):\n",
        "        encodings = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "        div_term = torch.exp(two_i * -(math.log(10000.0) / d_model))\n",
        "        encodings[:, 0::2] = torch.sin(position * div_term)\n",
        "        encodings[:, 1::2] = torch.cos(position * div_term)\n",
        "        encodings = encodings.unsqueeze(1).requires_grad_(False)\n",
        "        return encodings\n",
        "\n",
        "    def forward(self, x):\n",
        "        pe = self.positional_encoding[:x.shape[0]].detach().requires_grad_(False)\n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VpP2HyzNX3qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AddNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout):\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.ln(self.dropout(y), x)"
      ],
      "metadata": {
        "id": "wJH0y_aJajKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dropout):\n",
        "        self.lin1 = nn.Linear()\n",
        "        self.act = nn.ReLU()\n",
        "        self.lin2 = nn.Linear()"
      ],
      "metadata": {
        "id": "rb-l84rxaYNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, dropout):\n",
        "        # MultiheadAttention -> AddNorm -> FFN -> AddNorm\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        self.ffn = FeedForward(ffn_hiddens, d_model)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.addnorm1(x, self.attention(x, x, x))\n",
        "        x = self.addnorm2(x, self.ffn(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "XLtJ0hRWctF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(sequence_len, d_model, dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pos_endoding(self.embedding(x) * math.sqrt(self.d_model))\n",
        "        for blk in self.enc_blocks:\n",
        "            x = blk(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vhvPxGJNUJKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "BD4Xr41Scgyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_blocks, num_heads, dropout):\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(sequence_len, d_model, dropout)\n",
        "        self.dec_blocks = nn.Sequential(DecoderBlock(d_model, num_heads, dropout)\n",
        "                                        for _ in range(num_blocks)\n",
        "        self.lin = nn.Linear()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pos_encoding(self.embedding(x) * math.sqrt(self.d_model))\n",
        "        for blk in self.dec_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.lin(x)\n",
        "        return self.softmax(x)"
      ],
      "metadata": {
        "id": "rlYO81FGc_U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        self.encode = TransformerEncoder()\n",
        "        self.decode = TransformerDecoder()\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        self.decode(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mxnBbqSeEj1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerConfig:\n",
        "    d_model: int = 512\n",
        "    vocab_size: int = 50\n",
        "    sequence_len: int = 64\n",
        "    enc_dropout_prob: float = 0.5\n",
        "    dropout_prob: float = 0.2\n",
        "    ffn_hiddens: int = 48\n",
        "    num_blocks: int = 6\n",
        "    num_heads: int = 8\n",
        "    "
      ],
      "metadata": {
        "id": "0Lh2nqkiV9Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "config = TransformerConfig()\n",
        "net = Transformer(config)\n",
        "\n",
        "optimizer = torch.optim.SGD(net.parameters(), 1e-3)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x, y in zip(train_iter, labels):\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = net(x)\n",
        "        loss = lossfn(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        "
      ],
      "metadata": {
        "id": "R8KGUahaEzxr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}