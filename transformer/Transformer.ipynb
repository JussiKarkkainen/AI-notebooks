{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--O8l3gH7KR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"en_train.txt\", 'r') as f:\n",
        "    en_lines = f.readlines()\n",
        "with open(\"fr_train.txt\", 'r') as F:\n",
        "    fr_lines = F.readlines()"
      ],
      "metadata": {
        "id": "i9CyV2YoY2X-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(lines):\n",
        "    for i, line in enumerate(lines):\n",
        "        lines[i] = line.replace(' ', '')\n",
        "        lines[i] = line.strip()\n",
        "    tokens = [list(line) for line in lines]\n",
        "    return tokens\n",
        "\n",
        "en_token = get_tokens(en_lines)\n",
        "fr_token = get_tokens(fr_lines)\n",
        "en_token[:10]"
      ],
      "metadata": {
        "id": "kqXis5RrZMoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45de00ba-98a7-4add-ed23-ba052a604dd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['G', 'o', '.'],\n",
              " ['H', 'i', '.'],\n",
              " ['H', 'i', '.'],\n",
              " ['R', 'u', 'n', '!'],\n",
              " ['R', 'u', 'n', '!'],\n",
              " ['W', 'h', 'o', '?'],\n",
              " ['W', 'o', 'w', '!'],\n",
              " ['F', 'i', 'r', 'e', '!'],\n",
              " ['H', 'e', 'l', 'p', '!'],\n",
              " ['J', 'u', 'm', 'p', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(tokens):\n",
        "    return [items for i in tokens for items in i]\n",
        "\n",
        "en_tokens_flat = flatten(en_token)\n",
        "fr_tokens_flat = flatten(fr_token)\n",
        "print(len(en_tokens_flat))\n",
        "print(len(fr_tokens_flat))"
      ],
      "metadata": {
        "id": "7luSCs9BcSnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d8ef69-6218-4030-e991-9b8ee8f5e899"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3529268\n",
            "4243940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_char(tokens):\n",
        "    uniq_tokens = []\n",
        "    for i in tokens:\n",
        "        if i not in uniq_tokens:\n",
        "            uniq_tokens.append(i)\n",
        "    return uniq_tokens\n",
        "\n",
        "uniq_en_tokens = unique_char(en_tokens_flat)\n",
        "uniq_fr_tokens = unique_char(fr_tokens_flat)\n",
        "print(len(uniq_en_tokens))\n",
        "print(len(uniq_fr_tokens))\n",
        "print(uniq_en_tokens)"
      ],
      "metadata": {
        "id": "IoQJ0362cXDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2fe5170-dd11-4109-c8b4-3959d108a84f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84\n",
            "106\n",
            "['G', 'o', '.', 'H', 'i', 'R', 'u', 'n', '!', 'W', 'h', '?', 'w', 'F', 'r', 'e', 'l', 'p', 'J', 'm', 'S', 't', 'a', ' ', 'I', 's', 'y', 'O', 'A', 'c', 'k', 'C', 'g', 'f', 'd', \"'\", '1', '9', 'K', 'L', 'N', 'T', 'B', 'D', 'b', 'q', 'z', 'v', 'M', ',', 'P', 'Y', 'x', 'j', 'U', 'E', '$', '5', '3', ':', '0', '8', 'V', '7', '&', '%', '-', '2', 'Q', '6', '4', '\"', 'X', 'Z', 'é', '’', '€', '/', 'ç', '‘', 'а', '\\xad', '–', 'ö']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(unique_tokens):\n",
        "    vocab = {}\n",
        "    for e, char in enumerate(unique_tokens):\n",
        "        vocab[char] = (e + 1)\n",
        "    return vocab\n",
        "\n",
        "en_vocab = build_vocab(uniq_en_tokens)\n",
        "fr_vocab = build_vocab(uniq_fr_tokens)\n",
        "print(en_vocab)"
      ],
      "metadata": {
        "id": "1Wjh-c9scbJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb0d65e-6ff8-46da-866d-c4a255880658"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'G': 1, 'o': 2, '.': 3, 'H': 4, 'i': 5, 'R': 6, 'u': 7, 'n': 8, '!': 9, 'W': 10, 'h': 11, '?': 12, 'w': 13, 'F': 14, 'r': 15, 'e': 16, 'l': 17, 'p': 18, 'J': 19, 'm': 20, 'S': 21, 't': 22, 'a': 23, ' ': 24, 'I': 25, 's': 26, 'y': 27, 'O': 28, 'A': 29, 'c': 30, 'k': 31, 'C': 32, 'g': 33, 'f': 34, 'd': 35, \"'\": 36, '1': 37, '9': 38, 'K': 39, 'L': 40, 'N': 41, 'T': 42, 'B': 43, 'D': 44, 'b': 45, 'q': 46, 'z': 47, 'v': 48, 'M': 49, ',': 50, 'P': 51, 'Y': 52, 'x': 53, 'j': 54, 'U': 55, 'E': 56, '$': 57, '5': 58, '3': 59, ':': 60, '0': 61, '8': 62, 'V': 63, '7': 64, '&': 65, '%': 66, '-': 67, '2': 68, 'Q': 69, '6': 70, '4': 71, '\"': 72, 'X': 73, 'Z': 74, 'é': 75, '’': 76, '€': 77, '/': 78, 'ç': 79, '‘': 80, 'а': 81, '\\xad': 82, '–': 83, 'ö': 84}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_numerical(tokens, vocab):\n",
        "    new_lines = []\n",
        "    for line in tokens:\n",
        "        new_line = []\n",
        "        for char in line:\n",
        "            new_line.append(vocab[char])\n",
        "        new_lines.append(new_line)\n",
        "    return new_lines\n",
        "\n",
        "\n",
        "en_numerical = build_numerical(en_token, en_vocab)\n",
        "fr_numerical = build_numerical(fr_token, fr_vocab)\n",
        "en_numerical[:10]"
      ],
      "metadata": {
        "id": "buvkOpsncjEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c81135-2239-43fa-8d5a-82b8ebaca24d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3],\n",
              " [4, 5, 3],\n",
              " [4, 5, 3],\n",
              " [6, 7, 8, 9],\n",
              " [6, 7, 8, 9],\n",
              " [10, 11, 2, 12],\n",
              " [10, 2, 13, 9],\n",
              " [14, 5, 15, 16, 9],\n",
              " [4, 16, 17, 18, 9],\n",
              " [19, 7, 20, 18, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence(numerical, vocab):\n",
        "    def _get_max_len(numerical):\n",
        "        max = 0\n",
        "        for i in numerical:\n",
        "            length = len(i)\n",
        "            if length > max:\n",
        "                max = length\n",
        "        return max\n",
        "    \n",
        "    max_len = _get_max_len(numerical)\n",
        "    pad_token = 0\n",
        "    vocab[\"<PAD>\"] = pad_token\n",
        "    for i in numerical:\n",
        "        while len(i) < max_len:\n",
        "            i.append(pad_token)\n",
        "    return numerical, vocab\n",
        "\n",
        "padded_en_numerical, en_vocab = pad_sequence(en_numerical, en_vocab)\n",
        "padded_fr_numerical, fr_vocab = pad_sequence(fr_numerical, fr_vocab)"
      ],
      "metadata": {
        "id": "l3J7IMjMK3rU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = torch.tensor(src_data)\n",
        "        self.tgt_data = torch.tensor(tgt_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.tgt_data[idx]"
      ],
      "metadata": {
        "id": "o2lzXPjg4dBc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----- START OF TRANSFORMER -----"
      ],
      "metadata": {
        "id": "a4_XM-c9Ed54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TranslationDataset(en_numerical, fr_numerical)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "cy8YLBn35gKH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, sequence_len, d_model, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.sequence_len = sequence_len\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.register_buffer(\"positional_encoding\", self.get_pos_encoding(d_model, sequence_len), False)\n",
        "\n",
        "    def get_pos_encoding(self, d_model, max_len):\n",
        "        encodings = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "        div_term = torch.exp(two_i * -(math.log(10000.0) / d_model))\n",
        "        encodings[:, 0::2] = torch.sin(position * div_term)\n",
        "        encodings[:, 1::2] = torch.cos(position * div_term)\n",
        "        encodings = encodings.unsqueeze(1).requires_grad_(False)\n",
        "        return encodings\n",
        "\n",
        "    def forward(self, x):\n",
        "        pe = self.positional_encoding[:x.shape[0]].detach().requires_grad_(False)\n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VpP2HyzNX3qN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, d_model, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.ln(self.dropout(y) + x)"
      ],
      "metadata": {
        "id": "wJH0y_aJajKl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, ffn_hiddens, d_model):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_model, ffn_hiddens)\n",
        "        self.act = nn.ReLU()\n",
        "        self.lin2 = nn.Linear(ffn_hiddens, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.lin2(self.act(self.lin1(x)))"
      ],
      "metadata": {
        "id": "rb-l84rxaYNf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.value = nn.Linear(d_model, d_model, bias=True)\n",
        "        self.output = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = 1 / math.sqrt(self.d_k)\n",
        "        \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.shape[0]\n",
        "        q = self.query(q)\n",
        "        k = self.key(k)\n",
        "        v = self.value(v)\n",
        "        \n",
        "        Q = q.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        K = k.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        V = v.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = Q @ K.permute(0, 1, 3, 2)\n",
        "        scores *= self.scale\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        x = self.dropout(attn) @ V\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, -1, self.d_model)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1LXCEbbCmgDV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ffn_hiddens, dropout):\n",
        "        super().__init__()\n",
        "        # MultiheadAttention -> AddNorm -> FFN -> AddNorm\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        self.ffn = FeedForward(ffn_hiddens, d_model)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.addnorm1(x, self.attention(x, x, x, mask=src_mask))\n",
        "        x = self.addnorm2(x, self.ffn(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "XLtJ0hRWctF1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, sequence_len, num_heads, num_blocks, ffn_hiddens, dropout_prob):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(sequence_len, d_model, dropout_prob)\n",
        "        self.enc_blocks = nn.Sequential(*[EncoderBlock(d_model, num_heads, ffn_hiddens, dropout_prob)\n",
        "                                        for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # (batch_size, seq_len) (32, 64)\n",
        "        x = self.pos_encoding(self.embedding(x) * math.sqrt(self.d_model))\n",
        "        # (batch_size, seq_len, d_model) (32, 64, 512)\n",
        "        for blk in self.enc_blocks:\n",
        "            x = blk(x, src_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vhvPxGJNUJKV"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout, ffn_hiddens):\n",
        "        super().__init__()\n",
        "        self.mask_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        \n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "\n",
        "        self.ffn = FeedForward(ffn_hiddens, d_model)\n",
        "        self.addnorm3 = AddNorm(d_model, dropout)\n",
        "\n",
        "    def forward(self, dec, enc, trg_mask, src_mask):\n",
        "        attention = self.mask_attention(dec, dec, dec, mask=trg_mask)\n",
        "        _x = self.addnorm1(dec, attention)\n",
        "        x = self.enc_dec_attn(_x, enc, enc, mask=src_mask)\n",
        "        x = self.addnorm2(_x, x)\n",
        "        _x = self.ffn(x)\n",
        "        x = self.addnorm3(_x, x)\n",
        "        return x "
      ],
      "metadata": {
        "id": "BD4Xr41Scgyd"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, ffn_hiddens, num_blocks, num_heads, dropout, sequence_len):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(sequence_len, d_model, dropout)\n",
        "        self.dec_blocks = nn.Sequential(*[DecoderBlock(d_model, num_heads, dropout, ffn_hiddens)\n",
        "                                        for _ in range(num_blocks)])\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, trg, enc_out, trg_mask, src_mask):\n",
        "        x = self.pos_encoding(self.embedding(trg) * math.sqrt(self.d_model))\n",
        "        for blk in self.dec_blocks:\n",
        "            x = blk(x, enc_out, trg_mask, src_mask)\n",
        "        x = self.lin(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rlYO81FGc_U_"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(config.d_model, config.enc_vocab_size, config.sequence_len,\n",
        "                                         config.num_heads, config.num_blocks, config.ffn_hiddens, \n",
        "                                         config.enc_dropout_prob)\n",
        "        self.decoder = TransformerDecoder(config.dec_vocab_size, config.d_model, config.ffn_hiddens, config.num_blocks, \n",
        "                                         config.num_heads, config.dec_dropout_prob, config.sequence_len)\n",
        "\n",
        "    def encode(self, x, src_mask):\n",
        "        return self.encoder(x, src_mask)\n",
        "\n",
        "    def decode(self, trg, enc_out, trg_mask, src_mask):\n",
        "        return self.decoder(trg, enc_out, trg_mask, src_mask)\n",
        "    \n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != 0).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_subsequent_mask = torch.tril(torch.ones(trg_len, trg_len)).bool()\n",
        "        trg_mask = trg_pad_mask & trg_subsequent_mask\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_out = self.encode(src, src_mask)\n",
        "        x = self.decode(trg, enc_out, trg_mask, src_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mxnBbqSeEj1S"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerConfig:\n",
        "    d_model: int = 512\n",
        "    enc_vocab_size: int = 84\n",
        "    dec_vocab_size: int = 106\n",
        "    sequence_len: int = 83\n",
        "    enc_dropout_prob: float = 0.5\n",
        "    dec_dropout_prob: float = 0.5\n",
        "    dropout_prob: float = 0.2\n",
        "    ffn_hiddens: int = 48\n",
        "    num_blocks: int = 6\n",
        "    num_heads: int = 8\n",
        "    "
      ],
      "metadata": {
        "id": "0Lh2nqkiV9Z5"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "config = TransformerConfig()\n",
        "net = Transformer(config)\n",
        "lossfn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.SGD(net.parameters(), 1e-3)\n",
        "\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for x, y in dataloader:\n",
        "        # (batch_size, seq_len)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = net(x, y[:, :-1])\n",
        "        y_hat = y_hat.contiguous().view(-1, y_hat.shape[-1])\n",
        "        y = y[:, 1:].contiguous().view(-1)\n",
        "        loss = lossfn(y_hat, y)\n",
        "        print(f\"Loss is: {loss}\")\n",
        "        epoch_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Loss on epoch {epoch} was {epoch_loss.item}\")\n",
        "        "
      ],
      "metadata": {
        "id": "R8KGUahaEzxr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "8d184b42-810e-4820-c43d-d5164b935756"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss is: nan\n",
            "Loss is: nan\n",
            "Loss is: nan\n",
            "Loss is: nan\n",
            "Loss is: nan\n",
            "Loss is: nan\n",
            "Loss is: nan\n",
            "Loss is: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-8a748df26d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss is: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss on epoch {epoch} was {epoch_loss.item}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}