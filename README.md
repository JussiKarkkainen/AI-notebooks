# AI-notebooks

Implementations of various Neural Network architectures in [JAX/Haiku](https://github.com/deepmind/dm-haiku/tree/c18be3df5e85796492f2915af261b5517f12bacc) 
and [PyTorch](https://pytorch.org/).


## List of implemented models: WIP
- Transformer - [Attention is all you need](https://arxiv.org/abs/1706.03762) (PyTorch)
- ViT - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) (PyTorch)
- CLIP - [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (PyTorch)
- GPT - [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) (PyTorch)
- VAE - [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) (JAX/Haiku)
- ResNet-18 - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) (JAX/Haiku)
- LSTM - [Long Short-Term Memory](https://ieeexplore.ieee.org/abstract/document/6795963) (PyTorch)
- GRU - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259) (PyTorch)
- RNN - PyTorch 
