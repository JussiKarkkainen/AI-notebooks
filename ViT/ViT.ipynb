{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_w4YfpjfhKl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, img_size, \n",
        "                 patch_size):\n",
        "        super().__init__()\n",
        "        # Patchifying and linear map can be done with a convolution\n",
        "        def _make_tuple(x):\n",
        "            if not isinstance(x, (list, tuple)):\n",
        "                return (x, x)\n",
        "            return x\n",
        "        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)\n",
        "        self.num_patches = (img_size[0] // patch_size[0]) * (\n",
        "                            img_size[1] // patch_size[1])\n",
        "        self.conv = nn.Conv2d(in_channels, num_hiddens, kernel_size=patch_size,\n",
        "                              stride=patch_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Reshapes image of shape (N, C, H, W) to (N, Patches, C)\n",
        "        return self.conv(x).flatten(start_dim=2, end_dim=-1).transpose(1, 2)"
      ],
      "metadata": {
        "id": "6ORGaxmIqG5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTMLP(nn.Module):\n",
        "    def __init__(self, num_ins, mlp_num_hiddens, mlp_num_outputs, dropout):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(num_ins, mlp_num_hiddens)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.lin2 = nn.Linear(mlp_num_hiddens, mlp_num_outputs)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout2(self.lin2(self.dropout1(self.gelu(self.lin1(x)))))"
      ],
      "metadata": {
        "id": "omWwt2JlgGWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, num_ins, num_hiddens, norm_shape, \n",
        "                 mlp_num_hiddens, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.layer_norm1 = nn.LayerNorm(norm_shape)\n",
        "        self.attention = nn.MultiheadAttention(num_hiddens, num_heads, dropout)\n",
        "        self.layer_norm2 = nn.LayerNorm(norm_shape)\n",
        "        self.mlp = ViTMLP(num_ins, mlp_num_hiddens, num_hiddens, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ln = self.layer_norm1(x)\n",
        "        att, _ = self.attention(ln, ln, ln)\n",
        "        _x = att + x\n",
        "        ln2 = self.layer_norm2(_x)\n",
        "        out = self.mlp(ln2)\n",
        "        return out + _x"
      ],
      "metadata": {
        "id": "dmONeO2NkxgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens, \n",
        "                 num_heads, num_blks, emb_dropout, blk_dropout, num_ins,\n",
        "                 num_classes=10):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding(1, num_hiddens, img_size, patch_size)\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, num_hiddens))\n",
        "        num_steps = self.patch_embedding.num_patches + 1\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_steps, num_hiddens))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_blks):\n",
        "            self.blks.add_module(\"blk\"+str(i), ViTBlock(num_ins, num_hiddens,\n",
        "                                                        num_hiddens, mlp_num_hiddens, \n",
        "                                                        num_heads, blk_dropout))\n",
        "        self.head = nn.Sequential(nn.LayerNorm(num_hiddens), \n",
        "                                  nn.Linear(num_hiddens, num_classes))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # 1. Patch embedding + linear map\n",
        "        X = self.patch_embedding(x)\n",
        "        X = torch.cat((self.class_token.expand(X.shape[0], -1, -1), X), 1)\n",
        "        X = self.dropout(X + self.pos_embedding)\n",
        "        # 2. Attention\n",
        "        for blk in self.blks:\n",
        "            X = blk(X)\n",
        "        return self.head(X[:, 0])"
      ],
      "metadata": {
        "id": "uUPD2UWblFWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = ToTensor()\n",
        "\n",
        "train_set = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
      ],
      "metadata": {
        "id": "I8sICdUguK35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_hiddens, mlp_num_hiddens, num_heads, num_blks, num_ins = 256, 1024, 8, 1, 256\n",
        "emb_dropout, blk_dropout, lr = 0.2, 0.2, 0.1\n",
        "img_size, patch_size = 28, 7\n",
        "model = ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads, \n",
        "            num_blks, emb_dropout, blk_dropout, num_ins)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for X, y in train_loader:\n",
        "        y_hat = model(X)\n",
        "        loss = criterion(y_hat, y)\n",
        "        train_loss += loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"loss on epoch: {epoch} was {train_loss:.2f}\")"
      ],
      "metadata": {
        "id": "GWuVown2oT8x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}