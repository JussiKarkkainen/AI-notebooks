{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_w4YfpjfhKl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets.mnist import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "6ORGaxmIqG5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTMLP(nn.Module):\n",
        "    def __init__(self, num_ins, mlp_num_hiddens, mlp_num_outputs, dropout):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(num_ins, mlp_num_hiddens)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.lin2 = nn.Linear(mlp_num_hiddens, mlp_num_outputs)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout2(self.lin2(self.dropout1(self.gelu(self.lin1(x)))))"
      ],
      "metadata": {
        "id": "omWwt2JlgGWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, num_ins, embed_dims, num_hiddens, norm_shape, \n",
        "                 mlp_num_hiddens, mlp_num_outputs, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.layer_norm1 = nn.LayerNorm(norm_shape)\n",
        "        self.attention = nn.MultiheadAttention(embed_dims, num_heads, dropout)\n",
        "        self.layer_norm2 = nn.Layernorm(norm_shape)\n",
        "        self.mlp = ViTMLP(num_ins, mlp_num_hiddens, mlp_num_outputs, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ln = self.layer_norm1(x)\n",
        "        att = self.attention(ln, ln, ln)\n",
        "        _x = att + x\n",
        "        ln2 = self.layer_norm2(_x)\n",
        "        out = self.mlp(ln2)\n",
        "        out = out + _x"
      ],
      "metadata": {
        "id": "dmONeO2NkxgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads, num_blks,\n",
        "                 emb_dropout, blk_dropout):\n",
        "        super().__init__()\n",
        "        self.patch_embeddings = PatchEmbedding(img_size, patch_size, num_hiddens)\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_blks):\n",
        "            self.blks.add_module(\"blk\"+str(i), ViTBlock(num_ins, embed_dims, num_hiddens,\n",
        "                                                        mlp_num_hiddens, num_heads,\n",
        "                                                        blk_dropout)\n",
        "        self.head = nn.Sequential(nn.LayerNorm(num_hiddens), \n",
        "                                  nn.Linear(num_hiddens, num_classes))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        X = self.patch_embedding(X)\n",
        "        X = torch.cat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)\n",
        "        X = self.dropout(X + self.pos_embedding)\n",
        "        for blk in self.blks:\n",
        "            X = blk(X)\n",
        "        return self.head(X[:, 0])"
      ],
      "metadata": {
        "id": "uUPD2UWblFWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = ToTensor()\n",
        "\n",
        "train_set = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_set = MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
      ],
      "metadata": {
        "id": "I8sICdUguK35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "lr = 0.1\n",
        "optimizer = torch.optim.Adam(model.parameters, lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for X, y in train_loader:\n",
        "        y_hat = model(X)\n",
        "        loss = criterion(y_hat, y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"loss on epoch: {epoch} was {loss:.2f}\")"
      ],
      "metadata": {
        "id": "GWuVown2oT8x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}