{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install dm-haiku\n",
        "!pip3 install optax"
      ],
      "metadata": {
        "id": "g2WQiv49evdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oBqkQ_WfWOir"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import optax\n",
        "import numpy as np\n",
        "from typing import NamedTuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    transformn = transforms.Compose([transforms.Resize((224, 224)), transforms.Grayscale(3), # A way to get 3 channel MNIST\n",
        "                                     transforms.ToTensor()])\n",
        "    batch_size = 8\n",
        "    trainset = datasets.MNIST(root='./data', train=True,\n",
        "                              download=True, transform=transformn)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "    testset = datasets.MNIST(root='./data', train=False,\n",
        "                              download=True, transform=transformn)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                              shuffle=False, num_workers=2)\n",
        "\n",
        "    x_init = np.random.randn(16, 224, 224, 3).astype(np.float32)\n",
        "    return trainloader, x_init"
      ],
      "metadata": {
        "id": "1SMxQkbvXjA1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, out_channels, stride=1, use_1x1_conv=False):\n",
        "        super().__init__()\n",
        "        self.conv1 = hk.Conv2D(output_channels=out_channels, kernel_shape=3, stride=stride, padding='SAME')\n",
        "        self.bn1 = hk.BatchNorm(create_scale=True, create_offset=True, decay_rate=0.999)\n",
        "        self.conv2 = hk.Conv2D(output_channels=out_channels, kernel_shape=3, stride=1, padding='SAME')\n",
        "        if use_1x1_conv:\n",
        "            self.conv3 = hk.Conv2D(output_channels=out_channels, kernel_shape=1, stride=stride)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn2 = hk.BatchNorm(create_scale=False, create_offset=False, decay_rate=0.999)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = jax.nn.relu(self.bn1(self.conv1(x), is_training=True))\n",
        "        out = self.bn2(self.conv2(out), is_training=True)\n",
        "        if self.conv3:\n",
        "            x = self.conv3(x)\n",
        "        return jax.nn.relu((out + x))"
      ],
      "metadata": {
        "id": "elXIRaX3WVBY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = hk.Conv2D(output_channels=64, kernel_shape=7, stride=2, padding='SAME', name='conv1')\n",
        "        self.bn1 = hk.BatchNorm(create_scale=False, create_offset=False, decay_rate=0.999)    # 64\n",
        "        self.maxpool = hk.MaxPool(window_shape=3, strides=2, padding=1)\n",
        "        self.l1 = self._make_layer(64, 2, first_layer=True)\n",
        "        self.l2 = self._make_layer(128, 2)\n",
        "        self.l3 = self._make_layer(256, 2)\n",
        "        self.l4 = self._make_layer(512, 2)\n",
        "        self.avgpool = hk.AvgPool(window_shape=7, padding='SAME', strides=1)\n",
        "        self.fc = hk.Linear(10)\n",
        "        \n",
        "    def _make_layer(self, channels, num_blocks, first_layer=False):\n",
        "        layers = []\n",
        "        for b in range(num_blocks):\n",
        "            if b == 0 and not first_layer:\n",
        "                layers.append(ResNetBlock(channels, stride=2, use_1x1_conv=True))\n",
        "            else:\n",
        "                layers.append(ResNetBlock(channels))\n",
        "        return hk.Sequential(layers)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out = jax.nn.relu(self.bn1(self.conv1(x), is_training=True))\n",
        "        out = self.l1(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.l3(out)\n",
        "        out = self.l4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "lGs-FIQoWeDd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingState(NamedTuple):\n",
        "    params: hk.Params\n",
        "    opt_state: optax.OptState\n",
        "\n",
        "def net_fn(x):\n",
        "    return ResNet18()(x)\n",
        "\n",
        "lossfn = optax.softmax_cross_entropy_with_integer_labels\n",
        "\n",
        "def main():\n",
        "    network = hk.transform_with_state(net_fn)\n",
        "    optimizer = optax.sgd(1e-3)\n",
        "    init_rng = jax.random.PRNGKey(42)\n",
        "\n",
        "    def loss_fn(params, x, y, state, rng):\n",
        "        out, state = network.apply(params, state, rng, x)\n",
        "        loss = jnp.sum(lossfn(out, y))\n",
        "        return loss, state\n",
        "\n",
        "    def update_weights(training_state, x, y, state, rng):\n",
        "        (loss, grads), state = jax.value_and_grad(loss_fn, has_aux=True)(training_state.params, x, y, state, rng)\n",
        "        updates, opt_state = optimizer.update(grads, training_state.opt_state, params)\n",
        "        params = optimizer.apply_updates(params, updates)\n",
        "        return TrainingState(params, opt_state), loss, state\n",
        "\n",
        "    trainloader, x_init = load_dataset()\n",
        "    init_params, state = network.init(init_rng, x_init)\n",
        "    init_opt_state = optimizer.init(init_params)\n",
        "    training_state = TrainingState(params=init_params, opt_state=init_opt_state)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        epoch_loss = 0\n",
        "        for x, y in trainloader:\n",
        "            x = np.array(x.view(x.shape[0], x.shape[2], x.shape[3], x.shape[1])).astype(np.float32) # NHWC\n",
        "            y = np.array(y.view(y.shape[0], 1, 1))\n",
        "            training_state, loss, state = update_weights(training_state, x, y, state, init_rng)\n",
        "            epoch_loss += loss\n",
        "\n",
        "        print(f\"Loss on epoch: {epoch} was {epoch_loss}\")"
      ],
      "metadata": {
        "id": "DlN8AUh7WoNc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "v21x8sp_fI1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}